{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "7jutTVj_WG5w"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from nltk.util import bigrams\n",
        "from nltk.probability import FreqDist, ConditionalFreqDist\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "a1bPvDtBW1ot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " nltk.download('punkt')"
      ],
      "metadata": {
        "id": "zzejKA1UW3lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "PcQ5dI_4XPf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"houndOfBaskervilles.txt\",encoding='utf-8')\n",
        "wordslist = file.read().splitlines() # to escape \\n occurence\n",
        "wordslist = [i for i in wordslist if i!='']\n",
        "text = \"\"\n",
        "text = text.join(wordslist)"
      ],
      "metadata": {
        "id": "Mx-u-oOaXlGS"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "id": "Bn_XTS-SX84-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a string which has all the punctuations to be removed\n",
        "punctuations = '''!()-[]{};:'\"\\,<>./‘’?“”@#$%^&*_~'''\n",
        "cleantext = \"\"\n",
        "#remove chapter names\n",
        "chapter_pattern = r'Chapter [0-9]+.'\n",
        "text = re.sub(chapter_pattern, '', text)\n",
        "\n",
        "for char in text:\n",
        "    if char not in punctuations:\n",
        "        cleantext = cleantext + char\n",
        "\n",
        "#Converting the text into lower case\n",
        "cleantext = cleantext.lower()\n",
        "#remove chapter names\n",
        "\n"
      ],
      "metadata": {
        "id": "BrNDDS30X-3P"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(cleantext)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens_final = [i for i in tokens if not i in stop_words] # tokenising with removing stopwords\n",
        "finaltext = \"  \"\n",
        "finaltext = finaltext.join(tokens_final)#frequency distribution without stopwaords\n",
        "freq = nltk.FreqDist(tokens_final)\n",
        "freq = {k: v for k, v in sorted(freq.items(), key=lambda item: item[1],reverse=True)}\n",
        "x = list(freq.keys())[:40]\n",
        "y = list(freq.values())[:40]\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(x,y,c='r',lw=4,ls='-.')\n",
        "plt.grid()\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Token Frequency (without stopwords)',size=17)\n",
        "plt.xlabel('Words',size=14)\n",
        "plt.ylabel('Count',size=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Fcq5qfr0YFTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud = WordCloud(width = 800, height = 600,\n",
        "                background_color ='white',\n",
        "                min_font_size = 10,stopwords = {},colormap='winter').generate(finaltext)\n",
        "\n",
        "plt.figure(figsize = (12,8), facecolor = None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rf2kD4aPYU8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(cleantext)\n",
        "freq = nltk.FreqDist(tokens)\n",
        "freq = {k: v for k, v in sorted(freq.items(), key=lambda item: item[1],reverse=True)}\n",
        "x = list(freq.keys())[:40]\n",
        "y = list(freq.values())[:40]\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(x,y,c='r',lw=4,ls='-.')\n",
        "plt.grid()\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Token Frequency (with stopwords)',size=17)\n",
        "plt.xlabel('Words',size=14)\n",
        "plt.ylabel('Count',size=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gRaeeZ4OMoY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tagged = nltk.pos_tag(tokens)\n",
        "# tagged[:15] #first 15 POS tags"
      ],
      "metadata": {
        "id": "41bOKmcqYX_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "counts = Counter( tag for word,  tag in tagged)\n",
        "print(counts)"
      ],
      "metadata": {
        "id": "TqPPT26LYaOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq_tags = nltk.FreqDist(counts)\n",
        "freq_tags = {k: v for k, v in sorted(freq_tags.items(), key=lambda item: item[1],reverse=True)}\n",
        "x = list(freq_tags.keys())[:40]\n",
        "y = list(freq_tags.values())[:40]\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(x,y,c='r',lw=4,ls='-.')\n",
        "plt.grid()\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('TAGs Frequency',size=17)\n",
        "plt.xlabel('Tags',size=14)\n",
        "plt.ylabel('Count',size=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qEWjXZtuYcH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(r\"Chapter 9..txt\",encoding='utf-8')\n",
        "wordslist = file.read().splitlines() # to escape \\n occurence\n",
        "wordslist = [i for i in wordslist if i!='']\n",
        "text = \"\"\n",
        "text = text.join(wordslist)"
      ],
      "metadata": {
        "id": "1J2WJeS4MbkV"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuations = '''!()-[]{};:'\"\\,<>./‘’?“”@#$%^&*_~'''\n",
        "cleantext = \"\"\n",
        "\n",
        "chapter_pattern = r'Chapter [0-9]+.'\n",
        "text = re.sub(chapter_pattern, '', text)\n",
        "\n",
        "for char in text:\n",
        "    if char not in punctuations:\n",
        "        cleantext = cleantext + char\n",
        "\n",
        "#Converting the text into lower case\n",
        "cleantext = cleantext.lower()\n",
        "okens = word_tokenize(cleantext)"
      ],
      "metadata": {
        "id": "zEYNEA61Mhmt"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bi_grams = list(bigrams(tokens))\n",
        "\n",
        "# Calculate frequency distributions\n",
        "word_freq = FreqDist(tokens)\n",
        "bi_gram_freq = ConditionalFreqDist(bi_grams)\n",
        "bi_gram_freq['i']"
      ],
      "metadata": {
        "id": "LxxzANAuK6UJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "# Get a list of all unique words (conditions)\n",
        "conditions = list(bi_gram_freq.conditions())\n",
        "\n",
        "# Create a header row\n",
        "header = [' '] + conditions[:10]  # Include only the first 10 conditions\n",
        "\n",
        "# Create an empty matrix for storing the probabilities\n",
        "matrix = []\n",
        "\n",
        "# Iterate through the first 10 conditions and populate the matrix\n",
        "for condition in conditions[:10]:\n",
        "    row = [condition]\n",
        "    for word in conditions[:10]:\n",
        "        if word in bi_gram_freq[condition]:\n",
        "            probability = bi_gram_freq[condition][word] / word_freq[condition]\n",
        "            row.append(f\"{probability:.4f}\")\n",
        "        else:\n",
        "            row.append(\"0.0000\")\n",
        "    matrix.append(row)\n",
        "\n",
        "# Print the matrix\n",
        "\n",
        "print(tabulate(matrix, headers=header, tablefmt=\"fancy_grid\"))\n"
      ],
      "metadata": {
        "id": "baQcU0t9M2VI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word(previous_word):\n",
        "    if previous_word in bi_gram_freq:\n",
        "        next_word = bi_gram_freq[previous_word].max()\n",
        "        return next_word\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "id": "im1OO9TZNxoc"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"but _ was not the sight _ her body nor yet _ it that of the body _ Hugo Baskerville lying near _ which raised the hair _ the heads of _ three daredevil roysterers but _ was that standing _ Hugo and plucking at his throat, there stood a foul thing _ great black beast shaped like a hound yet larger than any hound _ ever mortal eye has rested _\"\n",
        "sentence_tokens = nltk.word_tokenize(sentence)\n",
        "sentence1 = \"but it was not the sight of her body nor yet was it that of the body of Hugo Baskerville lying near her which raised the hair upon the heads of these three daredevil roysterers but it was that standing over Hugo and plucking at his throat, there stood a foul thing a great black beast shaped like a hound yet larger than any hound that ever mortal eye has rested upon\"\n",
        "\n",
        "# Initialize a list to store the filled sentence\n",
        "filled_sentence = []\n",
        "\n",
        "for token in sentence_tokens:\n",
        "    if token == \"_\":\n",
        "        # Use the predict_next_word function to fill in the blank\n",
        "        previous_word = filled_sentence[-1] if filled_sentence else None\n",
        "        next_word = predict_next_word(previous_word)\n",
        "        if next_word:\n",
        "            filled_sentence.append(next_word)\n",
        "    else:\n",
        "        filled_sentence.append(token)\n",
        "\n",
        "# Combine the words to form the filled sentence\n",
        "filled_sentence_text = \" \".join(filled_sentence)\n",
        "print(\"Shanon Sentence:\", sentence)\n",
        "print(\"Original Sentence:\", sentence1)\n",
        "print(\"Filled Sentence:\", filled_sentence_text)"
      ],
      "metadata": {
        "id": "F595VqFsN27x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}